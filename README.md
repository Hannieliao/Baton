# BATON: Aligning Text-to-Audio Model with Human Preference Feedback

<a href="https://arxiv.org/abs/2402.00744"><img src="https://img.shields.io/badge/ArXiv-2402.00744-brightgreen"></a> 
<a href="https://baton2024.github.io/"><img src="https://img.shields.io/badge/Demo-BATON-purple"></a>
<a href="https://drive.google.com/drive/folders/1dzsvxn6XLcqhi19n2kzk1jzD1ZnlbM4T?usp=sharing"><img src="https://img.shields.io/badge/Dataset-Prefernce-blue"></a>

With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention. However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability. To alleviate this issue, we formulate the **BATON**, **the first framework specifically designed to enhance the alignment between generated audio and text prompt using human preference feedback**. Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback. 
Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs. Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model. The experiment results demonstrate that our BATON can significantly improve the generation quality of the original text-to-audio models, concerning audio integrity, temporal relationship, and alignment with human preference.

![Example Image](assets/pipeline.png)

## Todo List
- [x] üî• Release on arxiv!
- [x] üî• Release demo page!
- [x] üî• Release human perfence dataset!
- [x] üî• Release reward model training/inference code
- [ ] Release finetuning code based on tango

## Repository Overview
We provide an example of how you can Aligning Text-to-Audio Model with any of your specific preferences using Baton. And we are releasing our implementation of finetuning one of the SOTA text-to-audio model [Tango](https://github.com/declare-lab/tango), while supporting imporve more T2A models or other audio generation models.

* `configs`: Configuration files for training and evaluation.
* `data`: The audio for human annotation
    * `Audio`:
        * `2label_Integrity_HD_Audio`: Audio of 2 label prompt annotated by human
        * `2label_Integrity_RD_Audio`: Audio of 2 label prompt annotated by reward model
        * `3label_Temporal_HD_Audio`: Audio of 3 label prompt annotated by human
        * `3label_Temporal_HD_Audio`: Audio of 3 label prompt annotated by reward model
    * `HA`: Human annotation
    * `RA`: Reward model annotation
* `scripts`: Helper scripts
    * `finetune.sh`: Fine-tuning t2a Model
    * `inference_finetune.sh`: Inference using t2a model after fine-tuning
* `src`: Source code
    * `rewardmodel`: Reward model definitions
        * `RM_inference_2label_BCE.py`: Inference using reward model
        * `RM_inference_2label_Preference_BCE.py`: Inference using reward model
        * `RM_train_2label_BCE.py`: Train reward model using preference annotation
        * `RM_train_2label_Preference_BCE.py`: Train reward model using absolute annotation
    * `t2amodel`: T2A model(Coming soon)
        * `Tango-master(example)`: Any text-to-audio model
            * `finetune.py`: move under this path
            * `models.py`: move and replace origin one


## Quick Start
We provide an example of how you can Aligning Text-to-Audio Model with any of your specific preferences using Baton. And we take one of the SOTA text-to-audio model [Tango](https://github.com/declare-lab/tango) as example.


### Datasets

The 2.5k pretain data are random seleted from [AudioCaps](https://github.com/cdjkim/audiocaps) train set. The audio locations, corresponding captions and scores are specified in the *.json file in ./Data.

The audio for human annotation, generated by [Tango](https://github.com/declare-lab/tango) based on the checkpoint [Tango-Full-FT-AudioCaps](https://huggingface.co/declare-lab/tango-full-ft-audiocaps) by feeding 2 or 3 label prompts, is available in [Google drive](https://drive.google.com/drive/folders/1dzsvxn6XLcqhi19n2kzk1jzD1ZnlbM4T?usp=sharing).

**PD, HD and RD** separately represents for pretrain data, human and reward model labeled data.

For **Integrity** task, Download:

    2label_Integrity_HD_Audio and put it into /data
    2label_Integrity_RD_Audio and put it into /data

For **Temporal** task, Download:

    3label_Temporal_HD_Audio and put it into /data
    3label_Temporal_RD_Audio and put it into /data

### Fine-tuning T2A Model

We are releasing our implementation based on one of the SOTA T2A model [Tango](https://github.com/declare-lab/tango), based on the checkpoint [Tango-Full-FT-AudioCaps](https://huggingface.co/declare-lab/tango-full-ft-audiocaps). 

### Reward Model Training and Annotating

**Train Reward Model**
```python
cd RM
python RM_train_2label_BCE.py # For Integrity task
python RM_train_3label_BCE.py # For Temporal task
```

**Using Reward Model for Annotation**
```python
cd RM
python RM_inference_2label_BCE.py # For Integrity task
python RM_inference_3label_BCE.py # For Temporal task
```

## Acknowledgement
‚òÄÔ∏è We are truly indebted to the following outstanding open source work upon which we have based our work: [Tango](https://github.com/declare-lab/tango), [AudioLDM](https://github.com/haoheliu/AudioLDM-training-finetuning), and [audiocraft.metric](https://facebookresearch.github.io/audiocraft/api_docs/audiocraft/metrics/index.html).

## Citation

```
@article{liao2024baton,
  title={BATON: Aligning Text-to-Audio Model with Human Preference Feedback},
  author={Liao, Huan and Han, Haonan and Yang, Kai and Du, Tianjiao and Yang, Rui and Xu, Zunnan and Xu, Qinmei and Liu, Jingquan and Lu, Jiasheng and Li, Xiu},
  journal={arXiv preprint arXiv:2402.00744},
  year={2024}
}
```





